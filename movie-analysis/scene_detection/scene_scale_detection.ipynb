{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T12:45:47.225174200Z",
     "start_time": "2024-04-24T12:45:47.213895500Z"
    }
   },
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Process the Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T11:31:42.258082300Z",
     "start_time": "2024-04-24T11:23:31.566652800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4707 images are saved to train set;  1174 images are saved to test set.\n",
      "4024 images are saved to train set;  1008 images are saved to test set.\n",
      "5926 images are saved to train set;  1484 images are saved to test set.\n",
      "4065 images are saved to train set;  1014 images are saved to test set.\n",
      "6670 images are saved to train set;  1700 images are saved to test set.\n",
      "6387 images are saved to train set;  1620 images are saved to test set.\n"
     ]
    }
   ],
   "source": [
    "# todo: read the csv files and label the images\n",
    "\n",
    "# ```````````````````````````````````````#\n",
    "# 0 ->\tFS    \tForeground shot          #\n",
    "# 1\t->  ECU\t\tExtreme close up\t     #\n",
    "# 2\t->  CU\t\tClose up                 #\n",
    "# 3\t->  MCU\t\tMedium close up          #\n",
    "# 4\t->  MS \t\tMedium shot              #\n",
    "# 5\t->  MLS \tMedium long shot         #\n",
    "# 6\t->  LS \t\tLong shot                #\n",
    "# 7\t->  ELS \tExtreme long shot        #\n",
    "# 8\t->  INS \tInsert                   #\n",
    "# 9 ->  NA \t\tNot available            #\n",
    "# ```````````````````````````````````````#\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "#Fill in the maximum number of all files in each folder\n",
    "#======================================#\n",
    "cnt = [0,0,0,0,0,0,0,0,0,0]\n",
    "#======================================#\n",
    "\n",
    "\n",
    "def label_the_images(trainset_dir, testset_dir, csv_dir, images_dir):\n",
    "  \"\"\"\n",
    "  Based on the labels in the csv file, classify the images and save them.\n",
    "\n",
    "  parameters：\n",
    "      saveto_dir: the path to saving folder\n",
    "      csv_dir: the path to csv files (which saves the labels)\n",
    "      images_dir: the path to images \n",
    "  \"\"\"\n",
    "\n",
    "  df = pd.read_csv(csv_dir)\n",
    "  labels = df['shotscale']\n",
    "  images = []\n",
    "\n",
    "  for filename in os.listdir(images_dir):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "      image_path = os.path.join(images_dir, filename)\n",
    "      images.append(image_path)\n",
    "\n",
    "  shot_scale_map = {\n",
    "      0: 'FS',\n",
    "      1: 'ECU',\n",
    "      2: 'CU',\n",
    "      3: 'MCU',\n",
    "      4: 'MS',\n",
    "      5: 'MLS',\n",
    "      6: 'LS',\n",
    "      7: 'ELS',\n",
    "      8: 'INS',\n",
    "      9: 'NA'\n",
    "  }\n",
    "\n",
    "  train_data_num = 0\n",
    "  test_data_num = 0\n",
    "\n",
    "  for i, label in enumerate(labels):\n",
    "    if label == 9:\n",
    "        continue  # 跳过 \"NA\" 图像\n",
    "\n",
    "    if i >= len(images): \n",
    "        break\n",
    "    else:\n",
    "        name = images[i]\n",
    "        l = shot_scale_map[label]\n",
    "        cnt[label] += 1\n",
    "    \n",
    "        if random.random() <= 0.8:\n",
    "            saveto_dir = trainset_dir\n",
    "            train_data_num += 1\n",
    "        else:\n",
    "            saveto_dir = testset_dir\n",
    "            test_data_num += 1\n",
    "    \n",
    "        saveto = os.path.join(saveto_dir, l)\n",
    "        # os.makedirs(saveto, exist_ok=True)  # 如果目录不存在则创建\n",
    "    \n",
    "        new_name = f\"{cnt[label]:04d}\"  # 使用前导零格式化名称\n",
    "        new_path = os.path.join(saveto, f\"{new_name}.jpg\")\n",
    "        shutil.copy(name, new_path)\n",
    "          \n",
    "  print(train_data_num, 'images are saved to train set; ', test_data_num, 'images are saved to test set.')       \n",
    "    \n",
    "\n",
    "trainset_dir = './dataset/Classified/trainset'\n",
    "testset_dir = './dataset/Classified/testset'\n",
    "\n",
    "csv_dirs = ['./dataset/1950_antonioni_-_cronaca_di_un_amore/1950_antonioni_-_cronaca_di_un_amore.csv',\n",
    "            './dataset/1960_bergman_-_djavulens_oga/1960_bergman_-_djavulens_oga.csv',\n",
    "            './dataset/1969_fellini_-_satyricon/1969_fellini_-_satyricon.csv',\n",
    "            './dataset/1980_godard_-_sauve_qui_peut_la_vie/1980_godard_-_sauve_qui_peut_la_vie.csv',\n",
    "            './dataset/1990_scorsese_-_goodfellas/1990_scorsese_-_goodfellas.csv',\n",
    "            './dataset/2007_tarr_-_a_londoni_ferfi/2007_tarr_-_a_londoni_ferfi.csv']\n",
    "\n",
    "image_dirs = ['./dataset/1950_antonioni_-_cronaca_di_un_amore/images',\n",
    "              './dataset/1960_bergman_-_djavulens_oga/images',\n",
    "              './dataset/1969_fellini_-_satyricon/images',\n",
    "              './dataset/1980_godard_-_sauve_qui_peut_la_vie/images',\n",
    "              './dataset/1990_scorsese_-_goodfellas/images',\n",
    "              './dataset/2007_tarr_-_a_londoni_ferfi/images']\n",
    "\n",
    "# for csv_dir, image_dir in zip(csv_dirs, image_dirs):\n",
    "#     label_the_images(trainset_dir, testset_dir, csv_dir, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "output:\n",
    "4707 images are saved to train set;  1174 images are saved to test set.\n",
    "4024 images are saved to train set;  1008 images are saved to test set.\n",
    "5926 images are saved to train set;  1484 images are saved to test set.\n",
    "4065 images are saved to train set;  1014 images are saved to test set.\n",
    "6670 images are saved to train set;  1700 images are saved to test set.\n",
    "6387 images are saved to train set;  1620 images are saved to test set.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Trainset and Testset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T11:43:37.456914600Z",
     "start_time": "2024-04-24T11:43:37.270713300Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='./dataset/Classified/trainset', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='./dataset/Classified/testset', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#===--------===#\n",
    "batch_size = 784\n",
    "#===--------===#"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T11:43:37.791142200Z",
     "start_time": "2024-04-24T11:43:37.774262100Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T11:43:38.313128900Z",
     "start_time": "2024-04-24T11:43:38.287104600Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define VGG-16 Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T11:33:58.793639900Z",
     "start_time": "2024-04-24T11:33:58.777551900Z"
    }
   },
   "outputs": [],
   "source": [
    "# VGG16网络模型\n",
    "class Vgg16_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg16_net, self).__init__()\n",
    "\n",
    "        # 第一层卷积层\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            # 对64通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 对64通道特征图进行ReLU激活函数\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 输入64通道特征图，输出64通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            # 对64通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 对64通道特征图进行ReLU激活函数\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 进行2x2的最大池化操作，步长为2\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # 第二层卷积层\n",
    "        self.layer2 = nn.Sequential(\n",
    "            # 输入64通道特征图，输出128通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            # 对128通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(128),\n",
    "            # 对128通道特征图进行ReLU激活函数\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 输入128通道特征图，输出128通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            # 对128通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 进行2x2的最大池化操作，步长为2\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        # 第三层卷积层\n",
    "        self.layer3 = nn.Sequential(\n",
    "            # 输入为128通道，输出为256通道，卷积核大小为33，步长为1，填充大小为1\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # 批归一化\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            self.layer1,\n",
    "            self.layer2,\n",
    "            self.layer3,\n",
    "            self.layer4,\n",
    "            self.layer5\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # 对张量的拉平(flatten)操作，即将卷积层输出的张量转化为二维，全连接的输入尺寸为512\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "False\n",
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "print(torch.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T12:45:54.341773900Z",
     "start_time": "2024-04-24T12:45:54.322841800Z"
    }
   },
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T12:35:49.579846400Z",
     "start_time": "2024-04-24T12:35:42.845891300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU for computations\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 10070523904 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 49\u001B[0m\n\u001B[0;32m     47\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# 输入数据进行前向传播，行到预测结果\u001B[39;00m\n\u001B[1;32m---> 49\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# 计算损失\u001B[39;00m\n\u001B[0;32m     51\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[22], line 113\u001B[0m, in \u001B[0;36mVgg16_net.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 113\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv(x)\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;66;03m# 对张量的拉平(flatten)操作，即将卷积层输出的张量转化为二维，全连接的输入尺寸为512\u001B[39;00m\n\u001B[0;32m    115\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m512\u001B[39m)\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[1;32mD:\\Apps\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    457\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 10070523904 bytes."
     ]
    }
   ],
   "source": [
    "#\n",
    "batch_size = 16\n",
    "\n",
    "# 每n个batch打印一次损失\n",
    "num_print = 10\n",
    "\n",
    "# 总迭代次数\n",
    "epoch_num = 30\n",
    "# 初始学习率\n",
    "lr = 0.01\n",
    "# 每n次epoch更新一次学习率\n",
    "step_size = 10\n",
    "# 更新学习率每次减半\n",
    "gamma = 0.5\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use GPU\n",
    "    print(\"Using GPU for computations\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU\n",
    "    print(\"Using CPU for computations\")\n",
    "    \n",
    "# 创建Vgg16_net模型，并将其移动到GPU或CPU\n",
    "model = Vgg16_net().to(device)\n",
    "# 定义损失函数为交叉熵\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 定义优化器为随机梯度下降，学习率为lr，动量为0.8，权重衰减为0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.8, weight_decay=0.001)\n",
    "# 定义学习率调度器，每step_size次epoch将学习率减半\n",
    "schedule = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma, last_epoch=-1)\n",
    "\n",
    "# 训练\n",
    "loss_list = []\n",
    "\n",
    "# 每个epoch循环训练一遍\n",
    "for epoch in range(epoch_num):\n",
    "    # 当前迭代次数\n",
    "    ww = 0\n",
    "    # 累计损失值\n",
    "    running_loss = 0.0\n",
    "    # 遍历数据加载器，获取每个batch\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        # 将数据和标签移动到GPU/CPU上\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 输入数据进行前向传播，行到预测结果\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels).to(device)\n",
    "        # 反向传播，计算每个参数的梯度\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        # 累计损失值\n",
    "        running_loss += loss.item()\n",
    "        # 将损失值放到loss_list\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # 打印当前epoch的平均损失值\n",
    "        if (i + 1) % num_print == 0:\n",
    "            print('[%d epoch,%d]  loss:%.6f' % (epoch + 1, i + 1, running_loss / num_print))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    lr_1 = optimizer.param_groups[0]['lr']\n",
    "    # 打印学习率\n",
    "    print(\"learn_rate:%.15f\" % lr_1)\n",
    "    schedule.step()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
